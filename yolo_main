import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
import cv2
import os
import torch.optim as optim
from ultralytics import YOLO


# Detection head
class YOLOHead(nn.Module):
    def __init__(self, in_channels, grid_size, num_classes, num_anchors):
        super(YOLOHead, self).__init__()
        self.grid_size = grid_size
        self.num_classes = num_classes
        self.num_anchors = num_anchors

        # Hidden layer
        self.detector = nn.Sequential(
            nn.Conv2d(in_channels, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),

            # Output layer
            nn.Conv2d(256, num_anchors * (5 + num_classes), kernel_size=1)
        )

    def forward(self, x):
        return self.detector(x).permute(0, 2, 3, 1).contiguous()


class YOLO_Pretrained(nn.Module):   #nn.module is PyTorch base class for neural network
    def __init__(self, grid_size=7, num_classes=20, num_anchors=3, freeze_backbone=True):
        # super initialize parent class, required for Pytorch modules
        super(YOLO_Pretrained, self).__init__()

        yolo_model = YOLO('yolo11n.pt')

        # Gives access to the PyTorch nn.module which gives access to the neural network layers
        self.yolo_full_model = yolo_model.model

        self.grid_size = grid_size

        # Store features from backbone
        self.features = None

        self._register_hook()

        # Check how many channels final layer before head outputs
        self.backbone_out_channels = self._get_backbone_channels()

        # Freeze backbone, training only the detection head
        if freeze_backbone:
            for param in self.yolo_full_model.parameters():
                param.requires_grad = False # Dont compute the parameters

        # Resize feature maps forcing the output to be of fixed size regardless of input size, using average pooling
        self.adaptive_pool = nn.AdaptiveAvgPool2d((grid_size, grid_size))

        self.head = YOLOHead(self.backbone_out_channels, grid_size, num_classes, num_anchors)

    def _register_hook(self):
        # Runs during forward pass and intercepts data flowing through the network
        # Difficult to iterate through YOLOs architecture so this gives access to the final layer
        # before detection head
        def hook_fn(module, input, output):
            self.features = output

        target_layer = self.yolo_full_model.model[9]
        target_layer.register_forward_hook(hook_fn)

    def _get_backbone_channels(self):
        self.yolo_full_model.eval()

        # Fake input in order to test how many channels the layer before head outputs
        dummy_input = torch.randn(1, 3, 640, 640)

        with torch.no_grad():
            # Run forward pass to trigger hook in order to get the number of channels
            _ = self.yolo_full_model(dummy_input)

        if self.features is None:
            raise RuntimeError("Failed to capture features. Hook may not be registered correctly.")

        # Get the channel dimensions
        channels = self.features.shape[1]
        return channels

    def forward(self, x):   # x is the batch of images
        # Extract features during training
        # get the gradients during training but not during evaluation
        with torch.set_grad_enabled(self.training):
            _ = self.yolo_full_model(x) # Runs input through YOLO11n backbone
            features = self.features

        # Resize to desired grid size
        features = self.adaptive_pool(features)

        # Pass through custom head
        predictions = self.head(features)

        return predictions


class YOLODataset(Dataset):
    def __init__(self, img_dir, label_dir, img_size=640, grid_size=7, num_classes=20, num_anchors=3):
        self.img_dir = img_dir
        self.label_dir = label_dir
        self.img_size = img_size
        self.images = [f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]
        self.grid_size = grid_size
        self.num_classes = num_classes
        self.num_anchors = num_anchors

        self.transforms = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((img_size, img_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

    def __len__(self):  # PyTorch needs how many data samples
        return len(self.images)

    def __getitem__(self, idx):     # PyTorch needs to know how to load each sample
        img_path = os.path.join(self.img_dir, self.images[idx])
        label_path = os.path.join(self.label_dir,
                                  self.images[idx].replace(".jpg", ".txt").replace(".png", ".txt").replace(".jpeg",
                                                                                                           ".txt"))

        # Load image
        image = cv2.imread(img_path)
        if image is None:
            raise ValueError(f"Failed to load image: {img_path}")
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # Load bounding boxes
        boxes = []
        if os.path.exists(label_path):
            with open(label_path, "r") as f:
                for line in f.readlines():
                    values = line.strip().split()
                    if len(values) == 5:
                        class_label, x, y, w, h = map(float, values)
                        boxes.append([class_label, x, y, w, h])

        # Convert boxes to YOLO grid format
        target = self.encode_target(boxes)

        # Apply transforms
        image = self.transforms(image)

        return image, target

    def encode_target(self, boxes):
        # Creates an empty target grid
        target = torch.zeros((self.grid_size, self.grid_size, self.num_anchors * (5 + self.num_classes)))

        # Loop through each box
        for box in boxes:
            class_label, x_center, y_center, width, height = box
            class_label = int(class_label)

            # Find which grid cell the center falls into
            grid_x = int(x_center * self.grid_size)
            grid_y = int(y_center * self.grid_size)

            # Clamp to valid range, i.e if grid_size = 8 valid range is 0-7
            grid_x = min(grid_x, self.grid_size - 1)
            grid_y = min(grid_y, self.grid_size - 1)

            # Assign to first anchor, simplified code needs to be improved with IoU
            anchor_idx = 0
            offset = anchor_idx * (5 + self.num_classes)

            # Set box coordinates
            target[grid_y, grid_x, offset + 0] = x_center
            target[grid_y, grid_x, offset + 1] = y_center
            target[grid_y, grid_x, offset + 2] = width
            target[grid_y, grid_x, offset + 3] = height

            # Set confidence/objectness , i.e does this cell contain an object
            target[grid_y, grid_x, offset + 4] = 1.0

            # Set class (one-hot encoding)
            if class_label < self.num_classes:
                target[grid_y, grid_x, offset + 5 + class_label] = 1.0

        return target


def yolo_loss(predictions, targets, num_classes, lambda_coord=5, lambda_noobj=0.5):

    # Unpack predictions and targets, ... means all previous dimensions
    pred_boxes = predictions[..., :4]
    pred_conf = predictions[..., 4]
    pred_classes = predictions[..., 5:]
    target_boxes = targets[..., :4]
    target_conf = targets[..., 4]
    target_classes = targets[..., 5:]

    # Localization Loss , box coordinates
    mask = target_conf > 0      # Identify which cells contains objects
    if mask.sum() > 0:
        box_loss = lambda_coord * torch.sum((pred_boxes[mask] - target_boxes[mask]) ** 2)
    else:   # If the image have no objects skip calculations
        box_loss = torch.tensor(0.0, device=predictions.device)

    # Confidence Loss
    obj_loss = torch.sum((pred_conf[mask] - target_conf[mask]) ** 2) if mask.sum() > 0 else torch.tensor(0.0, device=predictions.device)

    # No object confidence loss, create no-object masks
    noobj_mask = target_conf == 0
    noobj_loss = lambda_noobj * torch.sum((pred_conf[noobj_mask]) ** 2)

    # Classification Loss
    class_loss = torch.sum((pred_classes[mask] - target_classes[mask]) ** 2) if mask.sum() > 0 else torch.tensor(0.0,
                                                                                                                 device=predictions.device)

    # Total Loss
    total_loss = box_loss + obj_loss + noobj_loss + class_loss
    return total_loss


if __name__ == "__main__":
    # Initialize dataset and dataloader
    print("Initializing dataset...")
    train_dataset = YOLODataset(
        img_dir="dataset_soccer/train/images",
        label_dir="dataset_soccer/train/labels",
        img_size=640,
        grid_size=7,
        num_classes=20,     # Ändra denna eventuellt, vi behöver se över labels och shit
        num_anchors=3
    )
    print(f"Dataset size: {len(train_dataset)}")

    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)

    # Initialize model with GPU if applicable
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    model = YOLO_Pretrained(
        grid_size=7,
        num_classes=20,
        num_anchors=3,
        freeze_backbone=True  # Set to False if you want to fine-tune the backbone too
    )
    model = model.to(device)

    # Optimizer - only train the head if backbone is frozen
    trainable_params = [p for p in model.parameters() if p.requires_grad]
    print(f"Trainable parameters: {sum(p.numel() for p in trainable_params):,}")
    optimizer = optim.Adam(trainable_params, lr=0.001)

    criterion = yolo_loss

    # Training loop
    num_epochs = 20
    for epoch in range(num_epochs):
        model.train()
        epoch_loss = 0

        for batch_idx, (images, targets) in enumerate(train_loader):
            images = images.to(device)
            targets = targets.to(device)

            # Forward pass
            predictions = model(images)

            # Loss calculation
            loss = criterion(predictions, targets, num_classes=20)

            # Backpropagation
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

            if batch_idx % 100 == 0:
                print(
                    f"Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}")

        avg_loss = epoch_loss / len(train_loader)
        print(f"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}")

        # Save checkpoint
        if (epoch + 1) % 5 == 0:
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_loss,
            }, f'checkpoint_epoch_{epoch + 1}.pth')
            print(f"Checkpoint saved: checkpoint_epoch_{epoch + 1}.pth")

    print("Training complete!")
